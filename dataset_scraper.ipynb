{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "26b77b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/miguel.d.ferrusca/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/miguel.d.ferrusca/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Library import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "import spacy\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2bfb692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(asin, pages=10):\n",
    "    \n",
    "    df = pd.DataFrame(columns=['rating', 'content', 'title'])\n",
    "    pages = range(1,pages+1)\n",
    "    asin=asin\n",
    "    \n",
    "    for page in pages:\n",
    "        \n",
    "        print('Scraped {} page(s). Scraping page {}. Total reviews: {}'.format(page-1, page, len(df)))\n",
    "        \n",
    "        # iterable url\n",
    "        url = 'https://www.amazon.com/product-reviews/{}/ref=cm_cr_getr_d_paging_btm_prev_1?pageNumber={}'.format(asin, page)\n",
    "        \n",
    "        # getting soup\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        page_source = driver.page_source\n",
    "        driver.quit()\n",
    "        soup = BeautifulSoup(page_source,'lxml')\n",
    "        \n",
    "        # parsing soup\n",
    "        reviews = soup.findAll(\"div\", {\"class\":\"a-section review aok-relative\"})\n",
    "        ## parsing reviews section\n",
    "        reviews = BeautifulSoup('<br/>'.join([str(tag) for tag in reviews]), 'html.parser')\n",
    "        \n",
    "        #getting title\n",
    "        title = soup.find(\"h1\", {\"class\":\"a-size-large a-text-ellipsis\"}).get_text()\n",
    "        \n",
    "        #getting content\n",
    "        contents = reviews.find_all(\"span\", {\"data-hook\":\"review-body\"})\n",
    "        content_lst = []\n",
    "        for content in contents:\n",
    "            text_ = content.find_all(\"span\")[0].get_text(\"\\n\").strip()\n",
    "            text_ = \". \".join(text_.splitlines())\n",
    "            text_ = re.sub(' +', ' ', text_)\n",
    "            content_lst.append(text_)\n",
    "            \n",
    "        #getting rating\n",
    "        ratings = reviews.find_all(\"i\", {\"data-hook\":\"review-star-rating\"})\n",
    "        full_rating_lst = []\n",
    "        for rating in ratings:\n",
    "            full_rating_lst.append(rating.find_all(\"span\")[0].contents[0])\n",
    "            \n",
    "        \n",
    "        rating_lst = []\n",
    "        for rating in full_rating_lst:\n",
    "            rating_lst.append(re.findall(\"\\d+\\.\\d+\", rating)[0])\n",
    "            \n",
    "            \n",
    "        # concatenating to main data frame\n",
    "        \n",
    "        temp_df = pd.DataFrame({'rating':rating_lst, 'content':content_lst, 'title':title})\n",
    "        df = df.append(temp_df)\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "481fd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "asins = ['B00GAC1D2G', 'B07RZ74VLR', \n",
    "         'B01LZNGPY3', 'B01H6GUCCQ', \n",
    "         'B07KXQX3S3', 'B07VGRJDFY', \n",
    "         'B07SFKTLZM', 'B071JRMKBH', \n",
    "         'B01NAWKYZ0', 'B08FC6C75Y',\n",
    "         'B01LWVX2RG', 'B01N1037CV', \n",
    "         'B08WWFWRY6', 'B07GBZ4Q68', \n",
    "         'B01N5OKGLH', 'B08H9M7LDY', \n",
    "         'B07SL6ZXBL', 'B08DF248LD', \n",
    "         'B010KYDNDG', 'B00NLZUM36',\n",
    "         'B08CVB3Y8B', 'B07Y693ND1', \n",
    "         'B01MY7GHKJ', 'B088GH4X9W',\n",
    "         'B07F7T8J9P', 'B093B218BN']\n",
    "\n",
    "asins = np.unique(asins).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bbd317f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 1\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 1\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 1\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 1\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 0\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 0\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 0\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 0\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 0\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 0\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 0\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 0\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 7\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 7\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 7\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 7\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.DataFrame(columns=['rating', 'content', 'title'])\n",
    "\n",
    "for asin in asins:\n",
    "    dataset = dataset.append(scraper(asin,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b47a9321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping products with high reviews\n",
    "\n",
    "reviews_by_product = dataset.groupby(dataset.title)['content'].agg('count').reset_index().sort_values(by='content', ascending=False)\n",
    "high_reviewed_prods = reviews_by_product[reviews_by_product['content']>=40]['title'].unique().tolist()\n",
    "\n",
    "clean_df = dataset[dataset['title'].isin(high_reviewed_prods)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ff738243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-65-364de5a1b6c4>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['polarity'] = [TextBlob(review).sentiment.polarity for review in clean_df['content'].tolist()]\n",
      "<ipython-input-65-364de5a1b6c4>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['subjectivity'] = [TextBlob(review).sentiment.subjectivity for review in clean_df['content'].tolist()]\n"
     ]
    }
   ],
   "source": [
    "# Getting text polarity and subjectivity\n",
    "\n",
    "clean_df['polarity'] = [TextBlob(review).sentiment.polarity for review in clean_df['content'].tolist()]\n",
    "clean_df['subjectivity'] = [TextBlob(review).sentiment.subjectivity for review in clean_df['content'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4ba33343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting dataset\n",
    "\n",
    "path = 'data/'\n",
    "\n",
    "clean_df.to_csv(os.path.join(path,r'dataset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24975557",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
